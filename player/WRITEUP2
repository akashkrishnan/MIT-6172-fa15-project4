SUMMARY

Our beta1_submission had a serial runtime (on input.txt) of 1.8 seconds on
awsrun. Our beta2_submission has a serial runtime of 0.74 seconds, which is a
2.5x speedup.


scout_search parallelization

Direct following the tips on parallelization did not gave any speed improvement.
Conversely, changing for to cilk_for in scout_search increased the time per
depth level ~6 times. The total amount of nodes considered were increased a lot,
but this does not give advantage in the game. Most of them would be pruned in
the sequential run of the algorithm.

The execution time was improved by coarse-graining the for loop. Manual
optimization showed, that the best performance holds when cilk_for parallel loop
executes 6 sequential for loops. Unfortunately, this parallelization technique
shows a big variance of execution time and the average is slightly bigger than
the best we’ve achieved without scout_search parallelization. So, for beta_2 we
decided not to include this improvement.


Combined laser_map and h_attackable

We merged the laser_map and h_attackable values together, which reduced the
runtime by roughly 20%.


Improved Incremental Sorting of Moves

We made a minor modification to how we sort the moves by scores in order to
reduce the number of times we had to sort the moves. This reduction in sorting
reduced the runtime by roughly 15%.


h_dist Lookup Table

We created a lookup table for the h_dist calculation. The table is of size
205 x 205 instead of 256 x 256 because 205 is the largest integer value of a
square. The usage of this lookup table reduced the runtime by roughly 15%.


Mutable Positions

The original implementation used immutable position values during the search
process. Whenever the search would make a possible move to evaluate, a new
position would be generated, which resulted in a lot of memcpy. We decided to
make the positions mutable, so that make_move would modify the current position
instead of creating a new position. This required us to implement an undo_move
function.

We implemented both the updated make_move and undo_move operations correctly and
tested it with the original make_move to see of calling undo_move immediately
after the mutating make_move would not alter the original program’s result.

However, making the position mutable caused us to make a huge number of
refactors to searchNode and the search functions, which took a very long time.
We ended up running out of time and not finishing the refactors, so the
beta2_submission does not have the very crucial improvement.

We expect the speedup to be in the order of 2x, meaning the serial runtime
should be around 0.4 seconds.


Endgame Tablebase

We created an endgame tablebase generator that would work backwards from a
winning position to find positions that guarantee a win if played perfectly.
We originally had the idea of storing the best possible move in any position
when the total number of pieces on the board was below a small number (5).
However, generating the table would take an extremely long time, but could be
reduced by running the generator on multiple 40-core AWS VMs. Regardless, the
storage requirement and the memory IO of such a table would be the absolute
bottleneck.

By only storing the positions and moves that guarantee a win, the endgame
tablebase would be substantially smaller, because as the number of pieces left
on the board increases, the ability to force a win decreases. Moreover, the
generation time is also substantially shorter due to memoization. We would
generate the tablebase for king vs king. Then, we would generate a tablebase for
king and pawn vs king, which would use the previously generated tablebase to
save time. This process of tablebase generation would incrementally proceed by
increasing the number of pieces for each opponent and trying all combinations of
such pieces.

Thus, during the search, if a position in the tablebase is reached, then the
search terminates, because the tablebase would provide the best possible move
and guide the bot to victory.

Unfortunately, due to time constraints and priorities in other aspects of this
project, we did not actually generate a usable tablebase; however, we plan on
using a generated tablebase for the final implementation.


Opening Book

Partially similar to the endgame tablebase, we discussed generating an opening
book. The opening book would not be exhaustive; instead, it would be sequences
of high-quality moves  made by both players for a small number of moves (8). The
purpose of the opening book would be to provide the bot a precomputed sequence
of moves that are known to be advantageous, which allows the bot to prune nodes
at a much higher rate. The opening book would be computed by having the bot
search as deep as possible in order to determine the best possible moves.
Unfortunately, due to time constraints, we did not generate or implement the
opening book.

